* Naming consensus:
- image name: {policy_number}_POLICY_{page_number}.jp*g where the page number is formatted with 3 leading zeros padding (001,002..)
- ocr coords name: image_name+"_coords.json"
- json doc true label name: {policy_number}_POLICY.json
- boxes and transcripts file: image_name+".tsv"
- entities file: image_name+".txt"

Final_pipeline folder structure analysis:

0. Pipeline settings:

Module summary : There is only one script in this folder, pipeline_settings.py. This module is responsible for setting most of the file paths which will be used in the further scripts
* All file paths (besides in model config) are set by the pipeline_settings.py script (just edit the root folder and everything will be saved in it).

Phases:

Analysis of all folders and scripts

1. Pre_OCR:

Module summary : These scripts should be used to prepare the files to be later used by the model and ocr system.
Pdf_to_image.py besides generating images from pdfs, already names the files according to the consensus and the files can later be flattened using
the combine_image_folders script. The image_name_preparation script should only be used for folders which were not generated by the pdf_to_image script and whose image files need to be 
renamed according to consensus.

Can be ran in any order according to needs(folder structure, format of source data ...)

pdf_to_image.py:
	- Input : Path to folder containing pdf files each named {policy_number}_POLICY.pdf. The script also expects 
	the poppler bin path which is used by the convert_from_path script in pdf2image package (Release-20.12.1\poppler-20.12.1\bin). This path is set in the pipeline_settings.py script
	- Output : Creates a new folder containing folders for each .pdf file with the same name. Each folder contains
	the extracted images from the document named {policy_number}_POLICY_{page_number} where page number is 
	formatted with 3 leading zeros padding, for example 001, 002...
	Goal : To extract jp*g files from .pdf files and name each file according to the consensus
combine_image_folders.py:
	- Input : Path to folder containing subfolders for each document, each containing the images for the document
	- Output : Creates a new folder where all of the images in the documents folders are copied (flattens and 
	deletes the subfolders)
	Goal : To flatten all images in a single folder from each subfolders where each refers to a separate document
image_name_preparation.py:
	- Input : Path to folder containing subfolders for each document where the images in the document are not
	already named according to the naming consensus. Expects that each subfolder is named {policy_number}_POLICY
	and that the images in the subfolder end with _{page_number}.
	- Output : Renames each image file in each subfolder according to the consensus {policy_number}_POLICY_{page_number}
	where page number is formatted with 3 leading zeros padding
	Goal : To ensure the naming of the files is according to the consensus


2. OCR:

Module summary: Performs OCR on images

2.1 image_to_coords.py:
	- Input: Path to folder containing images whose names are according to the consensus ({policy_number}_POLICY
_{page_number}). This is important since the page_number ocr attribute is set using the file name. Set by pipeline settings
	- Output: Creates a new folder with the ocr coordinates for each image, each file named: 
	{policy_number}_POLICY_{page_number}_coords.json where page number is formatted with 3 leading zeros. Out path is set in pipeline settings
* image_preprocessing.py and ocr_coordinates.py are used by the image_to_coords.py script
	Goal: To perform OCR on images with already prepped image names according to this consensus
	{policyNumber}_POLICY_{page_number} where page_number is some int (no need for padding, although it can have)


3. After_OCR_before_train:

Module summary: Creates necessary folder structure, generates initial tsv and entities files, splits into validation and training sets and prepares pick input

Should be ran in order

3.1 create_folders.py:
	- Input: Change the root_folder variable in the pipeline_settings.py. This script will create all the required
	subfolders and folder structure for the rest of the pipeline inside the root_folder
	- Output: Creates the needed folder structure. Root folder contains all other folders. Helper csv folder will contains
	the csv file generated by the image_doc_csv_generator.py script later on. Model input folder will contain the 
	split sets (Train and Valid) folders inside which will be generated with the split.py script later. Model output
	folder will contain model output files for images. Document level aggregated folder will contain aggregated
	document level model output. Label document level folder contains the doc level true jsons for the training files.
	Similarity output folder will contain a csv file similarity_output.csv generated by calc_similarity.py script later
	which is used by the aggregation logic. Report output folder will contain doc level report for the validation set.
	Boxes and transcripts for testing folder will contain refactored .tsv files for the validation set to be used as testing.
3.2 generate_tsv_files.py:
	- Input: Path to folder containing subfolders: images, coords, json. Images folders contains all images from
	all documents with names according to consensus. Coords folder contains all ocr files for all images. Json folder
	contains the true label json files for each document each named {policy_number}_POLICY.json. Set by pipeline_settings.py
	- Output: Creates a new folder with 2 subfolders: boxes_and_transcripts and entities. The boxes_and_transcripts
	folder contains the .tsv files for each image. The entities folder contains .txt files for each image with all
	labels in the image. Path set by pipeline_settings.py
	Goal: To generate the boxes and transcripts and entities files for all the images provided
* Here the DocViewer app can be used to relabel the files generated by the generate_tsv_files script. The app expects
the input to be folder containing subfolders: boxes_and_transcripts, entities, images and json. The boxes and transcripts
and entities folders are generated by the generate_tsv_files script, the images folder contains all images to be relabeled
and the json folder contains all doc level true json labels to be later changed by the app.
3.3 image_doc_csv_generator.py:
	- Input: Images folder containing all images with correct image names. Set by pipeline_settings.py
	- Output: Creates a .csv file which maps each image to the corresponding document. Set by pipeline_settings.py
	Goal: This script is later used for the split strategy, should be ran before split.py
3.4 split.py:
	- Input: Path to images dir, path to boxes and transcripts dir, path to entities dir and the size of the validation
	set (0-1). Set by pipeline_settings.py.
	- Output: Creates a new folder with subfolders Train and Valid, where the whole input data is copied to both
	folders with the difference being that in the train folder the train_samples_list.csv specifies which images
	will be used as training images and in the valid folder the valid_samples_list.csv specifies which images will
	be used as validaiton images. The split is done on document level (no document is split inside). Path set by pipeline_settings.py.
	Goal: To split the set of all files in training and validation sets
* coords_util.py is used by the generate_tsv_files script


4. TRAIN:
- In this phase, the model input is already prepared and located inside the model input folder in the settings.root_folder.
Here, the model config.json file should be modified: The variables files name in the validation_dataset and train_dataset
objects should be changed accordingly to navigate to the csv files in the Train and Valid folder. The save_dir attr. in the
trainer object should be modified to the path where the model is to be saved. Batch size, number of epochs, 
save period, early stopping and other parameters can also be modified according to needs.
Train command:  python train.py -c config.json -d 0 -dist false (in model root folder)


5. After_Train_before_Test:

Module summary: Refactor bt files, prepare validation set for testing

5.1 refactor_boxes_and_transcripts.py:
	-Output: Refactors the bt files for the validation dataset by removing the labels at the end, since training is
	finished. Saves refactored files in the Boxes and transcripts for testing folder from settings.py file.
	

6. TEST:
- In this phase, the model can be tested on new data.
Test command: python test.py -ckpt {path_to_best_saved_model.pth} --bt {path_to_boxes_and_transcripts_validation_files_folder(the refactored one)} 
--impt {path_to_validation_images_folder} --bs {batch_size} -g {device_num} -output {path_to_save_model_output(should be the same as the Model output
	folder from the pipeline settings)} (in model root folder)
Output: Creates output .txt file for each image and saves them in the -output folder


7. After_Test:

Module summary: Does preprocessing on the model output files, tries to aggregate the best results for a document and use the ocr files to infer missed fields

Should be ran in order

7.1 check_similarity.py:
	- Input: Requires the valid_samples_list.csv file, the model output folder, the ocr folder and the required fields (all set by pipeline_settings.py)
	- Output: Generates a similarity.csv file and saves it in the Similarity output folder created by the create_folders.py script. The file contains matches
	between the ocr segments and the model output segments with their similarity value and is later used by the aggregate_result.py script. Path set by pipeline_settings.py.
7.2 aggregate_result.py:
	- Input: Requires the similarity.csv file generated from the check_similarity.py script (set by pipeline_settings.py)
	- Output: Aggregates results from the model into document level results using the similarity file and each image output file for a document. Path set by pipeline_settings.py.
7.3 report_generator_doc_level.py:
	- Input: Uses the aggregated output files and compares them against the true labels (set by pipeline_settings.py)
	- Output: Generates a doc level report with metrics TP,TN,FP,FN for each field for all documents in the validation set. Path set by pipeline_settings.py.


***** Full_Pipeline:

Module summary: Contains summary aggregate functions for each bigger pipeline step. Should be used if you want to perform a bigger step and not run function by function separately

Aggregate pipeline scripts

- ocr.py: Runs the scripts: image_to_coords(2.1) and reports time spent (BEFORE INITIAL BT AND ENTITIES AND AFTER PDF CONVERSION AND SATYSFYING NAMING CONVENTION)
- gen_files.py: Runs the scripts: create_folders.py(3.1) and generate_tsv_files(3.2) and reports time spent (BEFORE DOCVIEWER APP)
- split_pipeline.py: Runs the scripts: image_doc_csv_generator.py(3.3) and split.py(3.4) and reports time spent (AFTER APP USAGE, BEFORE TRAIN)
- after_train.py: Runs the scripts: refactor_boxes_and_transcripts.py(5.1) (AFTER TRAIN)
- after_test.py: Runs the scripts: check_similarity.py(7.1), aggregate_result.py(7.2), report_generator_doc_level.py(7.3) and reports time spent (AFTER TEST)
- pipeline_run.py: Interface for running all the above aggregate scripts


* This pipeline can be used in two ways: 
- Separate scripts can be ran independently as long as the provided input to each script is correct
- Separate summary phases can be ran independently as long as the provided input to the first script of the phase is correct (via the pipeline_run interface)

* The pipeline doesn't automate the naming and pdf2images phases (doesn't integrate them in the automation) because of missing source data structural information


FULL USAGE SCENARIO:

* Enter Projects/vzd/PICK/PICK.Pytorch.Sroie folder and open terminal

AFTER OCR: 
* python final_pipeline_project/gen_files.py -> Will run create_folders.py and generate_tsv_files.py, creating needed folder
structure and creating initial bt files for all images in the folder initial_bt_ent in root folder

Here, the app should be used to relabel the dataset. Before that, copy the json label documents and the images folder
and put them in the initial_bt_ent folder then set this folder as the input dir to the DocViewer app. When relabeling, the
bt, entities and json files will be overwritten

AFTER RELABEL:
* python final_pipeline_project/split_pipeline.py -> Will run image_doc_csv_generator.py and split.py. IF YOU WANT TO
TOGGLE FILTER FOR DOCUMENTS WHICH DON'T HAVE FULL LABELS GO IN IMAGE_DOC_CSV_GENERATOR.PY AND COMMENT(will not filter)
/UNCOMMENT(will filter) LINES 25-41. Now training and validation data will be located in {root_folder}/model_input

AFTER SPLIT:
Train the model now. 
* First modify config.json for the attributes files_name in the train_dataset and validation_dataset
objects to point to model_input/Train/train_samples_list.csv and model_input/Valid/valid_samples_list.csv. Then:
* python train.py -c config.json -d 0 -dist false ; which will start training for epochs specified in config.json

AFTER TRAIN:
* python final_pipeline_project/after_train.py -> will refactor the boxes and transcripts for the validation folder

AFTER REFACTOR:
Test the model now.
* python test.py -ckpt /home/nca/Projects/vzd/PICK/Pick.Pytorch.Sroie/saved_real_data_final/models/PICK_Default/{find_best_model} 
--bt /home/nca/Projects/vzd/PICK/Pick.Pytorch.Sroie/data/try_no_1/boxes_and_transcripts_for_testing 
--impt /home/nca/Projects/vzd/PICK/Pick.Pytorch.Sroie/data/try_no_1/model_input/Valid/images --bs 4 -g 0 
-output /home/nca/Projects/vzd/PICK/Pick.Pytorch.Sroie/data/try_no_1/model_output 

AFTER TEST:
Postprocessing now.
* python final_pipeline_project/after_test.py -> this will generate a report and you can find the model output for all validation docs in the document_level_output_aggregated folder
and the per page model output in the model_output folder
